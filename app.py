#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from flask import Flask, render_template, request, jsonify, send_from_directory, Response, abort
import os
import csv
import subprocess
import shutil
from datetime import datetime
from pathlib import Path
import threading
import uuid
import time
import json
import queue
import re
from io import StringIO
import tempfile
import zipfile
import tarfile
import gzip
import py7zr
from collections import defaultdict
import sqlite3
from flask_socketio import SocketIO, emit, join_room, leave_room, rooms
import base64

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# å…¨åŸŸè®Šæ•¸å„²å­˜åˆ†æç‹€æ…‹
analysis_status = {}
analysis_streams = {}
user_comments = {}
chat_rooms = {}
online_users = {}

# FastGrep é…ç½®
config = {
    'keywords': {},
    'original_keywords': {},  # ä¿å­˜åŸå§‹é—œéµå­—ç”¨æ–¼å¾©åŸ
    'fastgrep_settings': {
        'threads': 4,
        'use_mmap': True,
        'context_lines': 0,
        'timeout': 120
    }
}

# åˆå§‹åŒ–è³‡æ–™åº«
def init_database():
    """åˆå§‹åŒ– SQLite è³‡æ–™åº«"""
    conn = sqlite3.connect('chat_data.db')
    cursor = conn.cursor()
    
    # èŠå¤©å®¤è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS chat_rooms (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            description TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            created_by TEXT
        )
    ''')
    
    # èŠå¤©è¨Šæ¯è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS chat_messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            room_id TEXT,
            user_name TEXT,
            message TEXT,
            message_type TEXT DEFAULT 'text',
            file_path TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (room_id) REFERENCES chat_rooms (id)
        )
    ''')
    
    # æŠ•ç¥¨è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS polls (
            id TEXT PRIMARY KEY,
            room_id TEXT,
            question TEXT,
            options TEXT,
            created_by TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (room_id) REFERENCES chat_rooms (id)
        )
    ''')
    
    # æŠ•ç¥¨çµæœè¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS poll_votes (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            poll_id TEXT,
            user_name TEXT,
            option_index INTEGER,
            voted_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (poll_id) REFERENCES polls (id)
        )
    ''')
    
    # ç”¨æˆ¶è©•è«–è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS user_comments (
            id TEXT PRIMARY KEY,
            analysis_id TEXT,
            module_name TEXT,
            file_path TEXT,
            comment TEXT,
            created_by TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    conn.commit()
    conn.close()

def check_fastgrep_available():
    """æª¢æŸ¥ fastgrep æ˜¯å¦å¯ç”¨ - å¼·å¥ç‰ˆæœ¬"""
    try:
        # å…ˆå˜—è©¦ fastgrep
        result = subprocess.run(['fastgrep', '--help'], 
                              capture_output=True, text=True, timeout=5)
        
        output = result.stdout + result.stderr
        fastgrep_indicators = ['é¸é …', 'usage', 'fastgrep', 'æ¨¡å¼', 'æª”æ¡ˆ']
        
        for indicator in fastgrep_indicators:
            if indicator in output.lower():
                print(f"âœ… fastgrep æª¢æŸ¥æˆåŠŸï¼Œæ‰¾åˆ°æŒ‡æ¨™: {indicator}")
                return True
        
        print(f"âŒ fastgrep è¼¸å‡ºä¸åŒ…å«é æœŸæŒ‡æ¨™ï¼Œä½¿ç”¨ grep æ›¿ä»£")
        return False
        
    except FileNotFoundError:
        print("âŒ fastgrep å‘½ä»¤æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ grep æ›¿ä»£")
        return False
    except subprocess.TimeoutExpired:
        print("âŒ fastgrep å‘½ä»¤è¶…æ™‚ï¼Œä½¿ç”¨ grep æ›¿ä»£")
        return False
    except Exception as e:
        print(f"âŒ fastgrep æª¢æŸ¥ç™¼ç”ŸéŒ¯èª¤: {str(e)}ï¼Œä½¿ç”¨ grep æ›¿ä»£")
        return False

def build_search_command(keyword, file_path, settings=None):
    """å»ºç«‹æœå°‹å‘½ä»¤ - å„ªå…ˆä½¿ç”¨ fastgrepï¼Œfallback åˆ° grep"""
    if settings is None:
        settings = config['fastgrep_settings']
    
    # ä½¿ç”¨é›™å¼•è™ŸåŒ…è£¹é—œéµå­—ç¢ºä¿æ­£ç¢ºæœå°‹
    quoted_keyword = f'"{keyword}"'
    
    if check_fastgrep_available():
        cmd = ['fastgrep']
        cmd.extend(['-n'])          # é¡¯ç¤ºè¡Œè™Ÿ
        cmd.extend(['-i'])          # å¿½ç•¥å¤§å°å¯«
        
        # åŸ·è¡Œç·’æ•¸é‡
        if settings.get('threads', 1) > 1:
            cmd.extend(['-j', str(settings['threads'])])
        
        # è¨˜æ†¶é«”æ˜ å°„
        if settings.get('use_mmap', False):
            cmd.extend(['-m'])
        
        # æœå°‹æ¨¡å¼ï¼ˆé—œéµå­—ï¼‰
        cmd.append(quoted_keyword)
        cmd.append(file_path)
    else:
        # ä½¿ç”¨æ¨™æº– grep ä½œç‚º fallback
        cmd = ['grep']
        cmd.extend(['-n'])          # é¡¯ç¤ºè¡Œè™Ÿ
        cmd.extend(['-i'])          # å¿½ç•¥å¤§å°å¯«
        cmd.extend(['-F'])          # å›ºå®šå­—ä¸²æœå°‹
        cmd.append(quoted_keyword)
        cmd.append(file_path)
    
    return cmd

def parse_search_output(output_lines, keyword, file_path):
    """è§£ææœå°‹è¼¸å‡º"""
    matches = []
    
    for line in output_lines:
        if not line.strip():
            continue
            
        # è¼¸å‡ºæ ¼å¼: line_number:content
        if ':' in line:
            first_colon = line.find(':')
            if first_colon > 0:
                line_number_str = line[:first_colon]
                content = line[first_colon + 1:] if first_colon + 1 < len(line) else ""
                
                try:
                    line_number = int(line_number_str)
                    if content.strip():
                        matches.append({
                            'filename': file_path,
                            'line_number': line_number,
                            'content': content.strip(),
                            'keyword': keyword
                        })
                except ValueError:
                    continue
    
    return matches

def extract_archive(file_path, extract_to):
    """è§£å£“ç¸®æª”æ¡ˆä¸¦è¿”å›æª”æ¡ˆåˆ—è¡¨"""
    extracted_files = []
    
    try:
        if file_path.endswith('.zip'):
            with zipfile.ZipFile(file_path, 'r') as zip_ref:
                zip_ref.extractall(extract_to)
                extracted_files = [os.path.join(extract_to, name) for name in zip_ref.namelist()]
        
        elif file_path.endswith('.tar.gz') or file_path.endswith('.tgz'):
            with tarfile.open(file_path, 'r:gz') as tar_ref:
                tar_ref.extractall(extract_to)
                extracted_files = [os.path.join(extract_to, name) for name in tar_ref.getnames()]
        
        elif file_path.endswith('.tar'):
            with tarfile.open(file_path, 'r') as tar_ref:
                tar_ref.extractall(extract_to)
                extracted_files = [os.path.join(extract_to, name) for name in tar_ref.getnames()]
        
        elif file_path.endswith('.gz'):
            output_file = os.path.join(extract_to, os.path.basename(file_path[:-3]))
            with gzip.open(file_path, 'rb') as gz_file:
                with open(output_file, 'wb') as out_file:
                    out_file.write(gz_file.read())
            extracted_files = [output_file]
        
        elif file_path.endswith('.7z'):
            with py7zr.SevenZipFile(file_path, mode='r') as z:
                z.extractall(path=extract_to)
                extracted_files = [os.path.join(extract_to, name) for name in z.getnames()]
        
        # éæ¿¾å‡ºå¯¦éš›çš„æª”æ¡ˆï¼ˆæ’é™¤ç›®éŒ„ï¼‰
        extracted_files = [f for f in extracted_files if os.path.isfile(f)]
        
    except Exception as e:
        print(f"è§£å£“ç¸®å¤±æ•—: {str(e)}")
        return []
    
    return extracted_files

def search_streaming(analysis_id, file_paths, keywords):
    """æµå¼æœå°‹"""
    try:
        status = analysis_status[analysis_id]
        stream_queue = analysis_streams[analysis_id]
        
        status['status'] = 'running'
        
        settings = config['fastgrep_settings']
        timeout = settings.get('timeout', 120)
        
        total_operations = len(file_paths) * sum(len(keyword_list) for keyword_list in keywords.values())
        completed_operations = 0
        
        # ç™¼é€é–‹å§‹è¨Šæ¯
        stream_queue.put({
            'type': 'start',
            'message': 'é–‹å§‹åˆ†æ...',
            'total_files': len(file_paths),
            'total_modules': len(keywords)
        })
        
        # åˆå§‹åŒ–çµæœçµæ§‹
        for module in keywords.keys():
            status['results'][module] = {
                'total_matches': 0,
                'files': {},
                'keywords_found': [],
                'search_time': 0,
                'errors': [],
                'processed_files': 0,
                'total_files': len(file_paths)
            }

        for module, keyword_list in keywords.items():
            status['current_module'] = module
            module_start_time = datetime.now()
            
            # ç™¼é€æ¨¡çµ„é–‹å§‹è¨Šæ¯
            stream_queue.put({
                'type': 'module_start',
                'module': module,
                'keywords': keyword_list
            })

            # ç‚ºæ¯å€‹æª”æ¡ˆåˆ†åˆ¥åŸ·è¡Œæœå°‹
            for file_path in file_paths:
                status['current_file'] = os.path.basename(file_path)
                
                # ç™¼é€æª”æ¡ˆé–‹å§‹è¨Šæ¯
                stream_queue.put({
                    'type': 'file_start',
                    'module': module,
                    'file': file_path
                })

                # è™•ç†è™›æ“¬æª”æ¡ˆè·¯å¾‘ï¼ˆä¸Šå‚³çš„æª”æ¡ˆï¼‰
                if file_path.startswith('/tmp/uploaded/'):
                    # è™•ç†ä¸Šå‚³çš„æª”æ¡ˆ
                    actual_path = os.path.join('uploads', os.path.basename(file_path))
                    if not os.path.exists(actual_path):
                        continue
                    file_path = actual_path

                if not os.path.exists(file_path) or not os.path.isfile(file_path):
                    error_msg = f"æª”æ¡ˆä¸å­˜åœ¨: {file_path}"
                    status['results'][module]['errors'].append(error_msg)
                    continue

                status['results'][module]['processed_files'] += 1

                for keyword in keyword_list:
                    try:
                        # å»ºç«‹æœå°‹å‘½ä»¤
                        cmd = build_search_command(keyword, file_path, settings)
                        
                        print(f"åŸ·è¡Œæœå°‹: {' '.join(cmd)}")
                        
                        # åŸ·è¡Œæœå°‹
                        process = subprocess.run(
                            cmd, 
                            capture_output=True, 
                            text=True, 
                            timeout=timeout,
                            encoding='utf-8',
                            errors='ignore'
                        )
                        
                        if process.returncode == 0:
                            # æ‰¾åˆ°åŒ¹é…
                            output_lines = process.stdout.strip().split('\n')
                            matches = parse_search_output(output_lines, keyword, file_path)
                            
                            # çµ„ç¹”çµæœä¸¦å³æ™‚ç™¼é€
                            if matches:
                                if file_path not in status['results'][module]['files']:
                                    status['results'][module]['files'][file_path] = []
                                
                                file_matches = []
                                for match in matches:
                                    match_data = {
                                        'line_number': match['line_number'],
                                        'content': match['content'],
                                        'keyword': match['keyword']
                                    }
                                    status['results'][module]['files'][file_path].append(match_data)
                                    file_matches.append(match_data)
                                    status['results'][module]['total_matches'] += 1
                                
                                if keyword not in status['results'][module]['keywords_found']:
                                    status['results'][module]['keywords_found'].append(keyword)

                                # å³æ™‚ç™¼é€åŒ¹é…çµæœ
                                stream_queue.put({
                                    'type': 'matches_found',
                                    'module': module,
                                    'file': file_path,
                                    'keyword': keyword,
                                    'matches': file_matches,
                                    'total_matches': status['results'][module]['total_matches']
                                })
                        
                        elif process.returncode == 1:
                            # æ²’æ‰¾åˆ°åŒ¹é…ï¼Œæ­£å¸¸æƒ…æ³
                            pass
                        else:
                            # å…¶ä»–éŒ¯èª¤
                            error_msg = process.stderr.strip() if process.stderr else f"è¿”å›ç¢¼: {process.returncode}"
                            status['results'][module]['errors'].append(f"æª”æ¡ˆ '{file_path}' é—œéµå­— '{keyword}': {error_msg}")
                        
                    except subprocess.TimeoutExpired:
                        error_msg = f"æª”æ¡ˆ '{file_path}' é—œéµå­— '{keyword}' è¶…æ™‚"
                        status['results'][module]['errors'].append(error_msg)
                    except Exception as e:
                        error_msg = f"æª”æ¡ˆ '{file_path}' é—œéµå­— '{keyword}' éŒ¯èª¤: {str(e)}"
                        status['results'][module]['errors'].append(error_msg)
                    
                    # æ›´æ–°é€²åº¦
                    completed_operations += 1
                    progress = min(int((completed_operations / total_operations) * 100), 100)
                    status['progress'] = progress
                    
                    # ç™¼é€é€²åº¦æ›´æ–°
                    stream_queue.put({
                        'type': 'progress',
                        'progress': progress,
                        'completed': completed_operations,
                        'total': total_operations
                    })
                    
                    time.sleep(0.01)

            # è¨ˆç®—æ¨¡çµ„æœå°‹æ™‚é–“
            module_end_time = datetime.now()
            status['results'][module]['search_time'] = (module_end_time - module_start_time).total_seconds()
            
            # ç™¼é€æ¨¡çµ„å®Œæˆè¨Šæ¯
            stream_queue.put({
                'type': 'module_complete',
                'module': module,
                'search_time': status['results'][module]['search_time'],
                'total_matches': status['results'][module]['total_matches']
            })
        
        # å®Œæˆåˆ†æ
        status['status'] = 'completed'
        status['progress'] = 100
        status['end_time'] = datetime.now()
        status['total_time'] = (status['end_time'] - status['start_time']).total_seconds()
        
        # ç™¼é€å®Œæˆè¨Šæ¯
        stream_queue.put({
            'type': 'complete',
            'total_time': status['total_time'],
            'total_matches': sum(module['total_matches'] for module in status['results'].values())
        })
        
    except Exception as e:
        status['status'] = 'error'
        status['error'] = str(e)
        status['progress'] = 100
        stream_queue.put({
            'type': 'error',
            'message': f"åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        })

def read_file_lines(file_path, target_line, context=200, start_line=None, end_line=None):
    """è®€å–æª”æ¡ˆæŒ‡å®šè¡Œæ•¸åŠå…¶ä¸Šä¸‹æ–‡ï¼Œæ”¯æ´è‡ªè¨‚ç¯„åœ"""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        
        total_lines = len(lines)
        
        # å¦‚æœæŒ‡å®šäº†èµ·å§‹å’ŒçµæŸè¡Œï¼Œä½¿ç”¨å®ƒå€‘
        if start_line is not None and end_line is not None:
            start_line = max(1, min(start_line, total_lines))
            end_line = min(total_lines, max(end_line, start_line))
        else:
            # å¦å‰‡ä½¿ç”¨ç›®æ¨™è¡Œå’Œä¸Šä¸‹æ–‡
            start_line = max(1, target_line - context)
            end_line = min(total_lines, target_line + context)
        
        result_lines = []
        for i in range(start_line - 1, end_line):
            result_lines.append({
                'line_number': i + 1,
                'content': lines[i].rstrip('\n\r'),
                'is_target': (i + 1) == target_line
            })
        
        return {
            'success': True,
            'lines': result_lines,
            'total_lines': total_lines,
            'start_line': start_line,
            'end_line': end_line,
            'target_line': target_line
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

def format_file_size(size_bytes):
    """æ ¼å¼åŒ–æª”æ¡ˆå¤§å°"""
    if size_bytes == 0:
        return "0 B"
    
    size_names = ["B", "KB", "MB", "GB", "TB"]
    import math
    i = int(math.floor(math.log(size_bytes, 1024)))
    p = math.pow(1024, i)
    s = round(size_bytes / p, 2)
    return f"{s} {size_names[i]}"

def generate_analysis_report(analysis_id):
    """ç”Ÿæˆåˆ†æå ±å‘Š"""
    if analysis_id not in analysis_status:
        return None
    
    status = analysis_status[analysis_id]
    results = status.get('results', {})
    
    # çµ±è¨ˆè³‡æ–™
    total_files = status.get('total_files', 0)
    total_matches = sum(module['total_matches'] for module in results.values())
    total_modules = len(results)
    
    # æŒ‰æ¨¡çµ„åˆ†çµ„çµæœ
    module_summary = []
    for module, data in results.items():
        module_summary.append({
            'name': module,
            'matches': data['total_matches'],
            'files_count': len(data['files']),
            'keywords_found': data['keywords_found'],
            'search_time': data['search_time']
        })
    
    # æŒ‰æª”æ¡ˆåˆ†çµ„çµæœ
    file_summary = defaultdict(lambda: {'modules': [], 'total_matches': 0})
    for module, data in results.items():
        for file_path, matches in data['files'].items():
            file_summary[file_path]['modules'].append({
                'module': module,
                'matches': len(matches),
                'keywords': list(set(match['keyword'] for match in matches))
            })
            file_summary[file_path]['total_matches'] += len(matches)
    
    return {
        'analysis_id': analysis_id,
        'summary': {
            'total_files': total_files,
            'total_matches': total_matches,
            'total_modules': total_modules,
            'analysis_time': status.get('total_time', 0)
        },
        'module_view': module_summary,
        'file_view': dict(file_summary),
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }

# WebSocket äº‹ä»¶è™•ç†
@socketio.on('connect')
def handle_connect():
    user_id = request.sid
    online_users[user_id] = {
        'name': f'ç”¨æˆ¶{len(online_users) + 1}',
        'joined_at': datetime.now()
    }
    emit('user_connected', {'user_id': user_id, 'online_count': len(online_users)})

@socketio.on('disconnect')
def handle_disconnect():
    user_id = request.sid
    if user_id in online_users:
        del online_users[user_id]
    emit('user_disconnected', {'user_id': user_id, 'online_count': len(online_users)}, broadcast=True)

@socketio.on('join_room')
def handle_join_room(data):
    room_id = data['room_id']
    user_name = data.get('user_name', f'ç”¨æˆ¶{request.sid[:8]}')
    
    join_room(room_id)
    online_users[request.sid]['name'] = user_name
    online_users[request.sid]['current_room'] = room_id
    
    emit('user_joined_room', {
        'user_name': user_name,
        'room_id': room_id
    }, room=room_id)

@socketio.on('leave_room')
def handle_leave_room(data):
    room_id = data['room_id']
    user_name = online_users.get(request.sid, {}).get('name', 'åŒ¿åç”¨æˆ¶')
    
    leave_room(room_id)
    if request.sid in online_users:
        online_users[request.sid].pop('current_room', None)
    
    emit('user_left_room', {
        'user_name': user_name,
        'room_id': room_id
    }, room=room_id)

@socketio.on('send_message')
def handle_send_message(data):
    room_id = data['room_id']
    message = data['message']
    user_name = online_users.get(request.sid, {}).get('name', 'åŒ¿åç”¨æˆ¶')
    
    # å„²å­˜åˆ°è³‡æ–™åº«
    conn = sqlite3.connect('chat_data.db')
    cursor = conn.cursor()
    cursor.execute('''
        INSERT INTO chat_messages (room_id, user_name, message, message_type)
        VALUES (?, ?, ?, ?)
    ''', (room_id, user_name, message, 'text'))
    conn.commit()
    conn.close()
    
    emit('new_message', {
        'user_name': user_name,
        'message': message,
        'timestamp': datetime.now().strftime('%H:%M:%S'),
        'room_id': room_id
    }, room=room_id)

# API è·¯ç”±
@app.route('/')
def index():
    return render_template('enhanced_index.html')

@app.route('/file_viewer')
def file_viewer():
    """æª¢è¦–æª”æ¡ˆ"""
    file_path = request.args.get('path')
    line_number = request.args.get('line', type=int, default=1)
    context = request.args.get('context', type=int, default=200)
    start_line = request.args.get('start', type=int)
    end_line = request.args.get('end', type=int)
    
    if not file_path or not os.path.exists(file_path):
        abort(404, 'æª”æ¡ˆä¸å­˜åœ¨')
    
    result = read_file_lines(file_path, line_number, context, start_line, end_line)
    
    if not result['success']:
        abort(500, f'è®€å–æª”æ¡ˆå¤±æ•—: {result["error"]}')
    
    return render_template('enhanced_file_viewer.html', 
                         file_path=file_path,
                         file_name=os.path.basename(file_path),
                         result=result)

@app.route('/analysis_report/<analysis_id>')
def analysis_report(analysis_id):
    """åˆ†æå ±å‘Šé é¢"""
    report_data = generate_analysis_report(analysis_id)
    if not report_data:
        abort(404, 'åˆ†æçµæœä¸å­˜åœ¨')
    
    return render_template('analysis_report.html', report=report_data)

@app.route('/chat')
def chat_page():
    """èŠå¤©å®¤é é¢"""
    return render_template('chat.html')

@app.route('/api/browse')
def browse_files():
    """ç€è¦½æª”æ¡ˆAPI"""
    path = request.args.get('path', '/home/vince_lin/Rust_Project')
    
    try:
        abs_path = os.path.abspath(path)
        
        if not os.path.exists(abs_path):
            return jsonify({'error': f'è·¯å¾‘ä¸å­˜åœ¨: {path}'})
        
        if not os.path.isdir(abs_path):
            return jsonify({'error': f'ä¸æ˜¯ç›®éŒ„: {path}'})
        
        items = []
        
        # æ·»åŠ è¿”å›ä¸Šç´šç›®éŒ„é¸é …
        if abs_path != '/':
            parent_path = os.path.dirname(abs_path)
            items.append({
                'name': '..',
                'path': parent_path,
                'type': 'directory',
                'is_parent': True,
                'size': '',
                'modified': ''
            })
        
        # åˆ—å‡ºç›®éŒ„å…§å®¹
        try:
            for item in sorted(os.listdir(abs_path)):
                if item.startswith('.'):
                    continue
                
                item_path = os.path.join(abs_path, item)
                
                try:
                    stat = os.stat(item_path)
                    modified = datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M')
                    
                    if os.path.isdir(item_path):
                        items.append({
                            'name': item,
                            'path': item_path,
                            'type': 'directory',
                            'is_parent': False,
                            'size': '',
                            'modified': modified
                        })
                    else:
                        size = format_file_size(stat.st_size)
                        items.append({
                            'name': item,
                            'path': item_path,
                            'type': 'file',
                            'is_parent': False,
                            'size': size,
                            'modified': modified
                        })
                except (OSError, PermissionError):
                    continue
                    
        except PermissionError:
            return jsonify({'error': f'æ²’æœ‰æ¬Šé™è¨ªå•: {path}'})
        
        return jsonify({
            'current_path': abs_path,
            'items': items
        })
        
    except Exception as e:
        return jsonify({'error': f'ç€è¦½ç›®éŒ„å¤±æ•—: {str(e)}'})

@app.route('/api/upload_keywords', methods=['POST'])
def upload_keywords():
    """ä¸Šå‚³é—œéµå­—æª”æ¡ˆ"""
    try:
        if 'file' not in request.files:
            return jsonify({'success': False, 'message': 'æ²’æœ‰é¸æ“‡æª”æ¡ˆ'})
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'success': False, 'message': 'æ²’æœ‰é¸æ“‡æª”æ¡ˆ'})
        
        if not file.filename.endswith('.csv'):
            return jsonify({'success': False, 'message': 'è«‹ä¸Šå‚³ CSV æª”æ¡ˆ'})
        
        # è®€å– CSV å…§å®¹
        content = file.read().decode('utf-8')
        csv_reader = csv.DictReader(content.splitlines())
        
        new_keywords = {}
        for row in csv_reader:
            module = None
            keyword_list = None
            
            # å˜—è©¦ä¸åŒçš„æ¬„ä½åç¨±çµ„åˆ
            for module_field in ['Module', 'module', 'æ¨¡çµ„', 'Category', 'category']:
                if module_field in row:
                    module = row[module_field].strip()
                    break
            
            for keyword_field in ['Keyword list', 'keyword list', 'Keywords', 'keywords', 'é—œéµå­—', 'é—œéµå­—æ¸…å–®']:
                if keyword_field in row:
                    keyword_list = row[keyword_field].strip()
                    break
            
            if module and keyword_list:
                # åˆ†å‰²é—œéµå­—
                keywords_split = []
                for separator in [',', ';', '\n']:
                    if separator in keyword_list:
                        keywords_split = [k.strip() for k in keyword_list.split(separator) if k.strip()]
                        break
                
                if not keywords_split:
                    keywords_split = [keyword_list]
                
                new_keywords[module] = keywords_split
        
        if not new_keywords:
            return jsonify({'success': False, 'message': 'CSV æª”æ¡ˆæ ¼å¼ä¸æ­£ç¢º'})
        
        # ä¿å­˜åŸå§‹é—œéµå­—ç”¨æ–¼å¾©åŸ
        config['original_keywords'] = new_keywords.copy()
        config['keywords'] = new_keywords
        
        return jsonify({
            'success': True,
            'message': f'æˆåŠŸè¼‰å…¥ {len(new_keywords)} å€‹æ¨¡çµ„çš„é—œéµå­—',
            'keywords': new_keywords
        })
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'è™•ç†æª”æ¡ˆå¤±æ•—: {str(e)}'})

@app.route('/api/keywords')
def get_keywords():
    """ç²å–ç•¶å‰é—œéµå­—"""
    return jsonify(config['keywords'])

@app.route('/api/keywords/delete/<module>', methods=['DELETE'])
def delete_keyword_module(module):
    """åˆªé™¤é—œéµå­—æ¨¡çµ„"""
    if module in config['keywords']:
        del config['keywords'][module]
        return jsonify({'success': True, 'message': f'å·²åˆªé™¤æ¨¡çµ„: {module}'})
    return jsonify({'success': False, 'message': 'æ¨¡çµ„ä¸å­˜åœ¨'})

@app.route('/api/keywords/restore', methods=['POST'])
def restore_keywords():
    """å¾©åŸæ‰€æœ‰é—œéµå­—æ¨¡çµ„"""
    config['keywords'] = config['original_keywords'].copy()
    return jsonify({
        'success': True, 
        'message': 'å·²å¾©åŸæ‰€æœ‰é—œéµå­—æ¨¡çµ„',
        'keywords': config['keywords']
    })

@app.route('/api/upload_archive', methods=['POST'])
def upload_archive():
    """ä¸Šå‚³ä¸¦è§£å£“ç¸®æª”æ¡ˆ"""
    try:
        if 'file' not in request.files:
            return jsonify({'success': False, 'message': 'æ²’æœ‰é¸æ“‡æª”æ¡ˆ'})
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'success': False, 'message': 'æ²’æœ‰é¸æ“‡æª”æ¡ˆ'})
        
        # æª¢æŸ¥æª”æ¡ˆæ ¼å¼
        allowed_extensions = ['.zip', '.7z', '.tar.gz', '.gz', '.tar']
        if not any(file.filename.endswith(ext) for ext in allowed_extensions):
            return jsonify({'success': False, 'message': 'ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼'})
        
        # å„²å­˜æª”æ¡ˆ
        upload_dir = 'uploads/archives'
        os.makedirs(upload_dir, exist_ok=True)
        file_path = os.path.join(upload_dir, file.filename)
        file.save(file_path)
        
        # è§£å£“ç¸®
        extract_dir = os.path.join(upload_dir, f"extracted_{uuid.uuid4().hex}")
        os.makedirs(extract_dir, exist_ok=True)
        
        extracted_files = extract_archive(file_path, extract_dir)
        
        # éæ¿¾æ—¥èªŒæª”æ¡ˆ
        log_files = []
        log_extensions = ['.log', '.txt', '.out', '.err']
        for file_path in extracted_files:
            if any(file_path.endswith(ext) for ext in log_extensions):
                log_files.append({
                    'name': os.path.basename(file_path),
                    'path': file_path,
                    'size': format_file_size(os.path.getsize(file_path))
                })
        
        return jsonify({
            'success': True,
            'message': f'æˆåŠŸè§£å£“ç¸®ï¼Œæ‰¾åˆ° {len(log_files)} å€‹æ—¥èªŒæª”æ¡ˆ',
            'files': log_files
        })
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'è™•ç†æª”æ¡ˆå¤±æ•—: {str(e)}'})

@app.route('/api/analyze_stream', methods=['POST'])
def analyze_stream():
    """å•Ÿå‹•æµå¼åˆ†æ"""
    try:
        data = request.get_json()
        selected_files = data.get('files', [])
        
        if not selected_files:
            return jsonify({'success': False, 'message': 'è«‹è‡³å°‘é¸æ“‡ä¸€å€‹æª”æ¡ˆ'})
        
        if not config['keywords']:
            return jsonify({'success': False, 'message': 'è«‹å…ˆä¸Šå‚³é—œéµå­—æ¸…å–®'})
        
        # é©—è­‰æª”æ¡ˆ
        valid_files = []
        for f in selected_files:
            if f.startswith('/tmp/uploaded/'):
                # è™•ç†ä¸Šå‚³çš„æª”æ¡ˆ
                actual_path = os.path.join('uploads', os.path.basename(f))
                if os.path.exists(actual_path):
                    valid_files.append(actual_path)
            elif os.path.exists(f) and os.path.isfile(f):
                valid_files.append(f)
        
        if not valid_files:
            return jsonify({'success': False, 'message': 'é¸æ“‡çš„æª”æ¡ˆéƒ½ä¸å­˜åœ¨æˆ–ç„¡æ³•è¨ªå•'})
        
        # å•Ÿå‹•æµå¼åˆ†æ
        analysis_id = str(uuid.uuid4())
        
        # å‰µå»ºæµå¼éšŠåˆ—
        stream_queue = queue.Queue()
        analysis_streams[analysis_id] = stream_queue
        
        # å„²å­˜åˆ†æç‹€æ…‹
        analysis_status[analysis_id] = {
            'status': 'started',
            'progress': 0,
            'total_files': len(valid_files),
            'total_modules': len(config['keywords']),
            'results': {},
            'current_file': '',
            'current_module': '',
            'start_time': datetime.now()
        }
        
        # åœ¨èƒŒæ™¯åŸ·è¡Œåˆ†æ
        thread = threading.Thread(
            target=search_streaming,
            args=(analysis_id, valid_files, config['keywords'])
        )
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'analysis_id': analysis_id,
            'message': 'é–‹å§‹æµå¼åˆ†æ',
            'valid_files_count': len(valid_files)
        })
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'åˆ†æéŒ¯èª¤: {str(e)}'})

@app.route('/api/analysis_stream/<analysis_id>')
def get_analysis_stream(analysis_id):
    """SSE æµå¼ç²å–åˆ†æçµæœ"""
    if analysis_id not in analysis_streams:
        abort(404, 'åˆ†æ ID ä¸å­˜åœ¨')
    
    def generate():
        stream_queue = analysis_streams[analysis_id]
        
        while True:
            try:
                message = stream_queue.get(timeout=5)
                yield f"data: {json.dumps(message, ensure_ascii=False)}\n\n"
                
                if message.get('type') in ['complete', 'error']:
                    break
                    
            except queue.Empty:
                yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
            except:
                break
    
    return Response(generate(), mimetype='text/event-stream')

@app.route('/api/user_comment', methods=['POST'])
def save_user_comment():
    """å„²å­˜ç”¨æˆ¶è©•è«–"""
    try:
        data = request.get_json()
        comment_id = str(uuid.uuid4())
        
        conn = sqlite3.connect('chat_data.db')
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO user_comments (id, analysis_id, module_name, file_path, comment, created_by)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (comment_id, data.get('analysis_id'), data.get('module_name'), 
              data.get('file_path'), data.get('comment'), data.get('user_name', 'åŒ¿å')))
        conn.commit()
        conn.close()
        
        return jsonify({'success': True, 'comment_id': comment_id})
        
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)})

@app.route('/api/user_comment/<comment_id>', methods=['GET', 'PUT', 'DELETE'])
def manage_user_comment(comment_id):
    """ç®¡ç†ç”¨æˆ¶è©•è«–"""
    conn = sqlite3.connect('chat_data.db')
    cursor = conn.cursor()
    
    try:
        if request.method == 'GET':
            cursor.execute('SELECT * FROM user_comments WHERE id = ?', (comment_id,))
            comment = cursor.fetchone()
            if comment:
                return jsonify({
                    'success': True,
                    'comment': {
                        'id': comment[0],
                        'analysis_id': comment[1],
                        'module_name': comment[2],
                        'file_path': comment[3],
                        'comment': comment[4],
                        'created_by': comment[5],
                        'created_at': comment[6]
                    }
                })
            return jsonify({'success': False, 'message': 'è©•è«–ä¸å­˜åœ¨'})
        
        elif request.method == 'PUT':
            data = request.get_json()
            cursor.execute('''
                UPDATE user_comments SET comment = ?, updated_at = CURRENT_TIMESTAMP 
                WHERE id = ?
            ''', (data.get('comment'), comment_id))
            conn.commit()
            return jsonify({'success': True, 'message': 'è©•è«–å·²æ›´æ–°'})
        
        elif request.method == 'DELETE':
            cursor.execute('DELETE FROM user_comments WHERE id = ?', (comment_id,))
            conn.commit()
            return jsonify({'success': True, 'message': 'è©•è«–å·²åˆªé™¤'})
            
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)})
    finally:
        conn.close()

@app.route('/download_sample')
def download_sample():
    """ä¸‹è¼‰ç¯„ä¾‹é—œéµå­—æª”æ¡ˆ"""
    try:
        sample_file = os.path.join('uploads', 'keywords_sample.csv')
        if os.path.exists(sample_file):
            return send_from_directory('uploads', 'keywords_sample.csv', as_attachment=True)
        
        # å‹•æ…‹ç”Ÿæˆç¯„ä¾‹æª”æ¡ˆ
        sample_content = """Module,Keyword list
Error,error,ERROR,failed,FAILED,exception,Exception,EXCEPTION
Warning,warning,WARN,alert,Alert,ALERT
Security,unauthorized,forbidden,attack,intrusion,hack,breach
Database,database error,connection failed,timeout,deadlock,query failed
Network,connection refused,network unreachable,dns resolution failed,timeout
System,out of memory,disk full,cpu usage,high load,memory leak
Authentication,login failed,authentication error,invalid credentials,access denied
Performance,slow query,timeout,high latency,performance degradation"""
        
        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8')
        temp_file.write(sample_content)
        temp_file.close()
        
        return send_from_directory(os.path.dirname(temp_file.name), 
                                 os.path.basename(temp_file.name), 
                                 as_attachment=True, 
                                 download_name='keywords_sample.csv')
    except Exception as e:
        return jsonify({'error': f'ä¸‹è¼‰ç¯„ä¾‹æª”æ¡ˆå¤±æ•—: {str(e)}'}), 500

if __name__ == '__main__':
    print("ğŸš€ Enhanced Log åˆ†æå¹³å° v5 å•Ÿå‹•ä¸­...")
    print("ğŸ†• æ–°å¢åŠŸèƒ½ï¼š")
    print("   - å¢å¼·æª”æ¡ˆæª¢è¦–å™¨ (æœå°‹ã€é«˜äº®ã€æ›¸ç±¤)")
    print("   - é—œéµå­—æ¨¡çµ„ç®¡ç† (åˆªé™¤/å¾©åŸ)")
    print("   - å£“ç¸®æª”æ¡ˆæ”¯æ´")
    print("   - èŠå¤©å®¤ç³»çµ±")
    print("   - ç”¨æˆ¶è©•è«–åŠŸèƒ½")
    print("   - åˆ†æå ±å‘Šç”Ÿæˆ")
    print("   - æŠ•ç¥¨å’Œè½‰ç›¤åŠŸèƒ½")
    print("=" * 50)
    
    # ç¢ºä¿å¿…è¦ç›®éŒ„å­˜åœ¨
    os.makedirs('uploads', exist_ok=True)
    os.makedirs('uploads/archives', exist_ok=True)
    
    # åˆå§‹åŒ–è³‡æ–™åº«
    init_database()
    
    socketio.run(app, debug=True, host='0.0.0.0', port=5000)